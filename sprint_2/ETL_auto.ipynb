{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data taxis verdes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcion sirve para realizar la carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_files(folder_path, Año):\n",
    "    parquet_files = Path(folder_path).glob('*.parquet')#Obtenemos una lista de rutas a archivos .parquet en la carpeta especificada\n",
    "    parquet_files_Año = [file for file in parquet_files if f'green_tripdata_{Año}' in file.name]#filtramos el nombre del archivo con el que vamos trabajar\n",
    "\n",
    "    data_frames_Año = []#iniciamos un lista para almacenar los DataFrames cargados desde los archivos .parquet\n",
    "    for file in parquet_files_Año:# Iteramos a través de los archivos .parquet correspondientes al año y cargar cada uno en un DataFrame\n",
    "        df = pd.read_parquet(file)# Cargamos el archivo parquet\n",
    "        data_frames_Año.append(df)# Agregamos el DataFrame cargado a la lista\n",
    "\n",
    "    return data_frames_Año # Devolvemos la lista de DataFrames correspondientes al año\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para transformar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_frames, Año):\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    combined_df['lpep_pickup_datetime'] = pd.to_datetime(combined_df['lpep_pickup_datetime'])\n",
    "    combined_df['pickup_date'] = combined_df['lpep_pickup_datetime'].dt.date\n",
    "\n",
    "    df_taxi = combined_df.copy()\n",
    "    df_taxi['pickup_date'] = pd.to_datetime(df_taxi['pickup_date'])\n",
    "\n",
    "    column_order = ['VendorID', 'pickup_date'] + [col for col in df_taxi.columns if col not in ['VendorID', 'pickup_date']]\n",
    "    df_taxi = df_taxi[column_order]\n",
    "\n",
    "    df_taxi = df_taxi[df_taxi['pickup_date'].dt.year == Año]\n",
    "\n",
    "    columns_to_remove = ['lpep_pickup_datetime','lpep_dropoff_datetime','RatecodeID','PULocationID','DOLocationID','passenger_count','trip_distance','fare_amount','payment_type','trip_type','extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'store_and_fwd_flag', 'congestion_surcharge']\n",
    "    df_taxi = df_taxi.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Agregar una columna que indica el día de la semana (0: Lunes, 6: Domingo)\n",
    "    df_taxi['weekday'] = df_taxi['pickup_date'].dt.weekday\n",
    "\n",
    "    # Mapear los números de día de la semana a nombres de días\n",
    "    day_names = ['Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo']\n",
    "    df_taxi['weekday'] = df_taxi['weekday'].apply(lambda x: day_names[x])\n",
    "\n",
    "    # Agrupar por fecha y calcular el total_amount por día\n",
    "    df_taxi_grouped = df_taxi.groupby(['pickup_date', 'weekday'])['total_amount'].sum().reset_index()\n",
    "\n",
    "    # Agregar una columna que indica la cantidad de veces que se repite una fecha\n",
    "    df_taxi_grouped['row_count'] = df_taxi['pickup_date'].value_counts()[df_taxi_grouped['pickup_date']].values\n",
    "\n",
    "    return df_taxi_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos los llamados a las funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = 'C:\\\\Users\\\\yopab\\\\Downloads\\\\Datasets\\\\dfexternos\\\\taxi amarillo'\n",
    "Año = 2023\n",
    "data_frames_Año = load_parquet_files(folder_path, Año)\n",
    "df_taxi_grouped = transform_data(data_frames_Año, Año)\n",
    "df_taxi_grouped.head(10)\n",
    "#Guardamos el DataFrame con el nombre correspondiente al año elegido\n",
    "df_taxi_grouped.to_csv(f'taxiV_{Año}.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Taxis amarillos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo procedimiento que en la funcion anterior, con la diferencia que al ser archivos mas pesados se deben crear DataFrame por día"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para la carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_filesY(folder_path, AñoY):\n",
    "    parquet_files = Path(folder_path_Y).glob('*.parquet')\n",
    "    parquet_files_Año = [file for file in parquet_files if f'yellow_tripdata_{AñoY}-0{mes}' in file.name]\n",
    "\n",
    "    data_frames_AñoY = []\n",
    "    for file in parquet_files_Año:\n",
    "        df = pd.read_parquet(file)\n",
    "        data_frames_AñoY.append(df)\n",
    "\n",
    "    return data_frames_AñoY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion de Transformación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_frames, AñoY):\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    combined_df['tpep_pickup_datetime'] = pd.to_datetime(combined_df['tpep_pickup_datetime'])\n",
    "    combined_df['pickup_date'] = combined_df['tpep_pickup_datetime'].dt.date\n",
    "\n",
    "    df_taxiY = combined_df.copy()\n",
    "    df_taxiY['pickup_date'] = pd.to_datetime(df_taxiY['pickup_date'])\n",
    "\n",
    "    column_order = ['VendorID', 'pickup_date'] + [col for col in df_taxiY.columns if col not in ['VendorID', 'pickup_date']]\n",
    "    df_taxiY = df_taxiY[column_order]\n",
    "\n",
    "    df_taxiY = df_taxiY[df_taxiY['pickup_date'].dt.year == AñoY]\n",
    "\n",
    "    columns_to_remove = ['VendorID', 'Airport_fee','tpep_pickup_datetime','tpep_dropoff_datetime','RatecodeID','PULocationID','DOLocationID','passenger_count','trip_distance','fare_amount','payment_type','extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'store_and_fwd_flag', 'congestion_surcharge']\n",
    "    df_taxiY = df_taxiY.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Agregar una columna que indica el día de la semana (0: Lunes, 6: Domingo)\n",
    "    df_taxiY['weekday'] = df_taxiY['pickup_date'].dt.weekday\n",
    "\n",
    "    # Mapear los números de día de la semana a nombres de días\n",
    "    day_names = ['Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo']\n",
    "    df_taxiY['weekday'] = df_taxiY['weekday'].apply(lambda x: day_names[x])\n",
    "\n",
    "    # Agrupar por fecha y calcular el total_amount por día\n",
    "    df_taxi_grouped = df_taxiY.groupby(['pickup_date', 'weekday'])['total_amount'].sum().reset_index()\n",
    "\n",
    "    # Agregar una columna que indica la cantidad de veces que se repite una fecha\n",
    "    df_taxi_grouped['row_count'] = df_taxiY.groupby('pickup_date').size().reset_index(name='row_count')['row_count']\n",
    "\n",
    "    return df_taxi_grouped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamado a las funciones y guardado de los DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_Y = 'C:\\\\Users\\\\yopab\\\\Downloads\\\\Datasets\\\\dfexternos\\\\taxi amarillo\\\\Amarillo'\n",
    "AñoY = 2023\n",
    "mes=5\n",
    "data_frames = load_parquet_filesY(folder_path_Y, AñoY)\n",
    "df_taxiY = transform_data(data_frames, AñoY)\n",
    "df_taxiY.head(10)\n",
    "df_taxiY.to_csv(f'taxiY_{AñoY}_{mes}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinacion de DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(folder_path):\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv') and f'taxiV_' in file]\n",
    "    \n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_frames.append(df)\n",
    "    \n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de transformación de DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_transform_data(data_frames):\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    columns_to_drop = [\"Unnamed: 0\"]\n",
    "    combined_df = combined_df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'C:\\\\Users\\\\yopab\\\\Downloads\\\\Datasets\\\\dfexternos\\\\taxi amarillo'\n",
    "\n",
    "data_frames = load_csv_files(folder_path)\n",
    "combined_data = combine_and_transform_data(data_frames)\n",
    "combined_data.to_csv('taxiG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv=pd.read_csv('taxiG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 884 entries, 0 to 883\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    884 non-null    int64  \n",
      " 1   pickup_date   884 non-null    object \n",
      " 2   weekday       884 non-null    object \n",
      " 3   total_amount  884 non-null    float64\n",
      " 4   row_count     884 non-null    int64  \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 34.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dfv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estaciones de carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfElectric=pd.read_csv('Electric and Alternative Fuel Charging Stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeo te lo que tiene dentro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfElectric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfElectric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformo el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtro por ubicación\n",
    "estaciones_ny = dfElectric[dfElectric['State'] == 'NY']\n",
    "# Lista de nombres de columnas a eliminar\n",
    "columnas_a_eliminar = ['Street Address','Intersection Directions','ZIP','Plus4','Station Phone','Status Code','Groups With Access Code',\n",
    "'Access Days Time','Cards Accepted','Date Last Confirmed','Updated At','Owner Type Code','Federal Agency ID',\n",
    "'Open Date','Country','Access Code','Facility Type','CNG On-Site Renewable Source','CNG Total Compression Capacity','CNG Storage Capacity','EV Pricing',\n",
    "'LPG Nozzle Types','CNG Fill Type Code','CNG PSI','EV On-Site Renewable Source','Restricted Access','Expected Date','BD Blends','NG Fill Type Code','NG PSI',\n",
    "'EV Other Info','EV Network Web','Hydrogen Status Link','LPG Primary', 'E85 Blender Pump', 'Intersection Directions (French)','Access Days Time (French)','BD Blends (French)',\n",
    "'Hydrogen Is Retail','Federal Agency Code','LNG On-Site Renewable Source','E85 Other Ethanol Blends','EV Pricing (French)','Hydrogen Pressures','Hydrogen Standards','Federal Agency Name'\n",
    "]\n",
    "\n",
    "# Elimino las columnas especificadas\n",
    "estaciones_ny = estaciones_ny.drop(columns=columnas_a_eliminar)\n",
    "# Reorganizo las columnas\n",
    "column_order = ['ID'] + [col for col in estaciones_ny.columns if col != 'ID']\n",
    "estaciones_ny=estaciones_ny[column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo el DataFrame creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones_ny.to_csv('Station_NY.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zona de Taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone=pd.read_csv('Dataset_extraidos\\\\taxi_zones.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviso la información que contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'OBJECTID']\n",
    "dftaxi_zone = dftaxi_zone.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['LocationID'] + [col for col in dftaxi_zone.columns if col != 'LocationID']\n",
    "dftaxi_zone=dftaxi_zone[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone = dftaxi_zone.rename(columns={'y': 'longitude', 'x': 'latitude'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone.to_csv('Taxi zone.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
