{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data taxis verdes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcion sirve para realizar la carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_files(folder_path, Año):\n",
    "    parquet_files = Path(folder_path).glob('*.parquet')#Obtenemos una lista de rutas a archivos .parquet en la carpeta especificada\n",
    "    parquet_files_Año = [file for file in parquet_files if f'green_tripdata_{Año}' in file.name]#filtramos el nombre del archivo con el que vamos trabajar\n",
    "\n",
    "    data_frames_Año = []#iniciamos un lista para almacenar los DataFrames cargados desde los archivos .parquet\n",
    "    for file in parquet_files_Año:# Iteramos a través de los archivos .parquet correspondientes al año y cargar cada uno en un DataFrame\n",
    "        df = pd.read_parquet(file)# Cargamos el archivo parquet\n",
    "        data_frames_Año.append(df)# Agregamos el DataFrame cargado a la lista\n",
    "\n",
    "    return data_frames_Año # Devolvemos la lista de DataFrames correspondientes al año\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para transformar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_frames, Año):\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)# Concatenamos los DataFrames de la lista en uno solo\n",
    "      # Convertimos columnas de fecha y hora de recogida y entrega a formato datetime\n",
    "    combined_df['lpep_pickup_datetime'] = pd.to_datetime(combined_df['lpep_pickup_datetime'])\n",
    "    combined_df['lpep_dropoff_datetime'] = pd.to_datetime(combined_df['lpep_dropoff_datetime'])\n",
    "    combined_df['pickup_date'] = combined_df['lpep_pickup_datetime'].dt.date# Creamos una nueva columna \"pickup_date\" con la fecha de recogida (sin la hora)\n",
    "        # Cambiamos las columna a formato datetime\n",
    "    df_taxi = combined_df.copy()\n",
    "    df_taxi['pickup_date'] = pd.to_datetime(df_taxi['pickup_date'])\n",
    "    df_taxi['lpep_pickup_datetime'] = pd.to_datetime(df_taxi['lpep_pickup_datetime'])\n",
    "    df_taxi['lpep_dropoff_datetime'] = pd.to_datetime(df_taxi['lpep_dropoff_datetime'])\n",
    "        # Ordemanos las columnas\n",
    "    column_order = ['VendorID', 'pickup_date'] + [col for col in df_taxi.columns if col not in ['VendorID', 'pickup_date']]\n",
    "    df_taxi = df_taxi[column_order]\n",
    "     # Filtramos el DataFrame para incluir solo registros del año especificado\n",
    "    df_taxi = df_taxi[df_taxi['pickup_date'].dt.year == Año]\n",
    "    #Eliminamos las columnas que no son reelevantes para nuestro DataFrame\n",
    "    columns_to_remove = ['extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'store_and_fwd_flag', 'congestion_surcharge']\n",
    "    df_taxi = df_taxi.drop(columns=columns_to_remove)\n",
    "\n",
    "    return df_taxi #Nos devuelve el Dataframe ya listo para guardarse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos los llamados a las funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021-01-01 00:15:56</td>\n",
       "      <td>2021-01-01 00:19:52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "      <td>151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID pickup_date lpep_pickup_datetime lpep_dropoff_datetime  \\\n",
       "0         2  2021-01-01  2021-01-01 00:15:56   2021-01-01 00:19:52   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0         1.0            43           151              1.0           1.01   \n",
       "\n",
       "   fare_amount  total_amount  payment_type  trip_type  \n",
       "0          5.5           6.8           2.0        1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "folder_path = 'C:\\\\Users\\\\yopab\\\\Downloads\\\\Datasets\\\\dfexternos\\\\taxi amarillo'\n",
    "Año = 2021\n",
    "data_frames_Año = load_parquet_files(folder_path, Año)\n",
    "df_taxiG= transform_data(data_frames_Año, Año)\n",
    "df_taxiG.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el DataFrame con el nombre correspondiente al año elegido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxiG.to_csv(f'taxiV_{Año}.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Taxis amarillos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo procedimiento que en la funcion anterior, con la diferencia que al ser archivos mas pesados se deben crear DataFrame por mes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para la carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_filesY(folder_path, AñoY):\n",
    "    parquet_files = Path(folder_path_Y).glob('*.parquet')\n",
    "    parquet_files_Año = [file for file in parquet_files if f'yellow_tripdata_{AñoY}-{mes}' in file.name]\n",
    "\n",
    "    data_frames_AñoY = []\n",
    "    for file in parquet_files_Año:\n",
    "        df = pd.read_parquet(file)\n",
    "        data_frames_AñoY.append(df)\n",
    "\n",
    "    return data_frames_AñoY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion de Transformación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data_frames, AñoY):\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    combined_df['tpep_pickup_datetime'] = pd.to_datetime(combined_df['tpep_pickup_datetime'])\n",
    "    combined_df['tpep_dropoff_datetime'] = pd.to_datetime(combined_df['tpep_dropoff_datetime'])\n",
    "    combined_df['pickup_date'] = combined_df['tpep_pickup_datetime'].dt.date\n",
    "\n",
    "    df_taxiY = combined_df.copy()\n",
    "    df_taxiY['pickup_date'] = pd.to_datetime(df_taxiY['pickup_date'])\n",
    "    df_taxiY['tpep_pickup_datetime'] = pd.to_datetime(df_taxiY['tpep_pickup_datetime'])\n",
    "    df_taxiY['tpep_dropoff_datetime'] = pd.to_datetime(df_taxiY['tpep_dropoff_datetime'])\n",
    "\n",
    "    column_order = ['VendorID', 'pickup_date'] + [col for col in df_taxiY.columns if col not in ['VendorID', 'pickup_date']]\n",
    "    df_taxiY = df_taxiY[column_order]\n",
    "\n",
    "    df_taxiY = df_taxiY[df_taxiY['pickup_date'].dt.year == AñoY]\n",
    "\n",
    "    columns_to_remove = ['extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'store_and_fwd_flag', 'congestion_surcharge']\n",
    "    df_taxiY = df_taxiY.drop(columns=columns_to_remove)\n",
    "\n",
    "    return df_taxiY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamado a las funciones y guardado de los DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3252711 entries, 0 to 3252716\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int64         \n",
      " 1   pickup_date            datetime64[ns]\n",
      " 2   tpep_pickup_datetime   datetime64[ns]\n",
      " 3   tpep_dropoff_datetime  datetime64[ns]\n",
      " 4   passenger_count        float64       \n",
      " 5   trip_distance          float64       \n",
      " 6   RatecodeID             float64       \n",
      " 7   PULocationID           int64         \n",
      " 8   DOLocationID           int64         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  total_amount           float64       \n",
      " 12  airport_fee            float64       \n",
      "dtypes: datetime64[ns](3), float64(6), int64(4)\n",
      "memory usage: 347.4 MB\n"
     ]
    }
   ],
   "source": [
    "folder_path_Y = 'C:\\\\Users\\\\yopab\\\\Downloads\\\\Datasets\\\\dfexternos\\\\taxi amarillo\\\\Amarillo'\n",
    "AñoY = 2022\n",
    "mes=12\n",
    "data_frames = load_parquet_filesY(folder_path_Y, AñoY)\n",
    "df_taxiY = transform_data(data_frames, AñoY)\n",
    "df_taxiY.info()\n",
    "df_taxiY.to_csv(f'taxiY_{AñoY}_{mes}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinacion de DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_files(folder_path, year):\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv') and f'taxiY_{year}' in file]\n",
    "    \n",
    "    data_frames = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_frames.append(df)\n",
    "    \n",
    "    return data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de transformación de DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_transform_data(data_frames):\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'C:\\\\Users\\\\yopab\\\\Downloads\\\\Datasets\\\\dfexternos\\\\taxi amarillo'\n",
    "year = 2022\n",
    "\n",
    "data_frames = load_csv_files(folder_path, year)\n",
    "combined_data = combine_and_transform_data(data_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('taxiY_{year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNo se puede ejecutar el código, la sesión se ha eliminado. Intente reiniciar el kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl Kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. Revise el código de las celdas para identificar una posible causa del error. Haga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. Vea el [registro] de Jupyter (command:jupyter.viewOutput) para obtener más detalles."
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('taxiY_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfElectric=pd.read_csv('Electric and Alternative Fuel Charging Stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfElectric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfElectric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones_ny = dfElectric[dfElectric['State'] == 'NY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de nombres de columnas a eliminar\n",
    "columnas_a_eliminar = ['Street Address','Intersection Directions','ZIP','Plus4','Station Phone','Status Code','Groups With Access Code',\n",
    "'Access Days Time','Cards Accepted','Date Last Confirmed','Updated At','Owner Type Code','Federal Agency ID',\n",
    "'Open Date','Country','Access Code','Facility Type','CNG On-Site Renewable Source','CNG Total Compression Capacity','CNG Storage Capacity','EV Pricing',\n",
    "'LPG Nozzle Types','CNG Fill Type Code','CNG PSI','EV On-Site Renewable Source','Restricted Access','Expected Date','BD Blends','NG Fill Type Code','NG PSI',\n",
    "'EV Other Info','EV Network Web','Hydrogen Status Link','LPG Primary', 'E85 Blender Pump', 'Intersection Directions (French)','Access Days Time (French)','BD Blends (French)',\n",
    "'Hydrogen Is Retail','Federal Agency Code','LNG On-Site Renewable Source','E85 Other Ethanol Blends','EV Pricing (French)','Hydrogen Pressures','Hydrogen Standards','Federal Agency Name'\n",
    "]\n",
    "\n",
    "# Elimina las columnas especificadas\n",
    "estaciones_ny = estaciones_ny.drop(columns=columnas_a_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganiza las columnas\n",
    "column_order = ['ID'] + [col for col in estaciones_ny.columns if col != 'ID']\n",
    "estaciones_ny=estaciones_ny[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones_ny.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones_ny.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones_ny.to_csv('Station_NY.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone=pd.read_csv('Dataset_extraidos\\\\taxi_zones.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'OBJECTID']\n",
    "dftaxi_zone = dftaxi_zone.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = ['LocationID'] + [col for col in dftaxi_zone.columns if col != 'LocationID']\n",
    "dftaxi_zone=dftaxi_zone[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone = dftaxi_zone.rename(columns={'y': 'longitude', 'x': 'latitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftaxi_zone.to_csv('Taxi zone.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
